---
layout: page
title: Research
---

<head>
  <title>Research overview</title>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} });
  </script>
  
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
  <style>
	.project-title {
	  font-size: 20px;
	  display:inline-block;vertical-align:top;
	}
  .column {
    float: left;
    width: 50%;
    padding: 5px;
    margin-bottom: none;
    padding-bottom: none;
  }
  .row {
    margin-bottom: none;
    padding-bottom: none;
  }
  /* Clear floats after image containers */
  .row::after {
    content: "";
    clear: both;
    display: table;
  }
  </style>

<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<link rel="stylesheet" href="{{ site.baseurl }}/assets/css/academicons.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  
</head>
<body>

The overarching objective of my research work is to establish provable safety guarantees for autonomous systems. 
During my PhD I focused on establishing safety guarantees for systems losing control over some of their actuators, which led to my resilience framework discussed below.
As a postdoctoral scholar I am interested in ensuring the safety of black-box systems controlled by reinforcement learning or diffusion models.


<hr style="margin-top:1cm;">
<h2>DDAT: Diffusion Planning of Dynamically Feasible Robot Trajectories
  <a class="button-link" href="https://iconlab.negarmehr.com/DDAT/" target="_blank" rel="noopener noreferrer">
    <span class="button"><i class="fas fa-globe"></i> Website</span>
  </a>
</h2>

<p>
  Diffusion planners can generate an entire trajectory in 1-shot, but their predicted sequences of states have no feasibility guarantees.
  A transition $s_t$ to $s_{t+1}$ is <em>dynamically feasible</em> if and only if there exists an action $a_t$ transforming state $s_t$ into $s_{t+1}$ by the robot's dynamics.
  To generate only dynamically feasible trajectories I devised several projection algorithms counteracting compounding projection errors and I incorporate these projectors into the training and inference of a diffusion transformer (DiT).
</p> 
<img src="{{site.baseurl}}/assets/gifs/DDAT_GO2_comparison.gif" alt="animated" class="center_img"/>
<div class="caption">
  Unitree GO2 <strong>zero-shot</strong> hardware deployment of <strong>open-loop</strong> trajectories generated by a vanilla diffusion policy (left) and our DDAT model (right).<br>
	The vanilla diffusion policy fails at walking through the cones in open-loop.<br>
  By accounting for the quadruped's dynamics our open-loop diffusion policy succeeds in following the corridor.
</div>	



<hr style="margin-top:1cm;">
<h2>POLICEd RL: Guaranteeing Safety in Reinforcement Learning
  <a class="button-link" href="https://iconlab.negarmehr.com/POLICEd-RL/" target="_blank" rel="noopener noreferrer">
    <span class="button"><i class="fas fa-globe"></i> Website</span>
  </a>
</h2>
  
<p>
  In this work I developed provable safety guarantees in Reinforcement Learning (RL).
	More specifically, I enforce hard constraints on a learned policy in closed-loop with a black-box environment.
	In <a href="{{ site.baseurl }}/assets/blog_posts/safe_RL" target="_blank" rel="noopener noreferrer">this blog post</a> I discuss how most of the safe RL literature <em>cannot guarantee such constraint respect</em>.
	Based on this observation, I devised <strong>POLICEd RL</strong> to solve this issue.
</p>
<div class="row">
  <div class="column">
    <img src="{{ site.baseurl }}/assets/gifs/POLICEd_RL.gif" alt="" style="width:100%">
  </div>
  <div class="column">
    <img src="{{ site.baseurl }}/assets/gifs/NonConstrainedKUKA.gif" alt=" " style="width:100%">
  </div>
</div>
<div class="caption">
  The POLICEd policy (left) is affine in the buffer region (cyan) and avoids the constraint (red) but not the baseline (right).
</div>


<p>
	The main idea for POLICEd RL is to make the policy repulsive in the state-space region surrounding the constraint.
	This repulsive buffer will then push trajectories away from the constraint and guarantee its satisfaction.
	The policy learns to be repulsive as it receives penalties for each constraint violation during training.
	My key insight is to use the <a href="https://ieeexplore.ieee.org/document/10096520" target="_blank" rel="noopener noreferrer">POLICE algorithm</a>
	to make the policy affine in this buffer, which then allows to verify its repulsive character by only evaluating the policy at the vertices of the buffer.
	This discrete and small number of evaluations to <em>guarantee safety</em> contrasts with the typical approach of learning a safety certificate that needs to be
	evaluated <em>everywhere on the state space</em> to check if this learned certificate verifies the analytical safety conditions.
  </p>
  <p>
	We illustrate POLICEd RL below on a simple 2D system and show that safety is guaranteed at the end of training.
	Without the affine policy, TD3 requires several orders of magnitude more training episodes to learn a policy pointing away from the constraint line on its whole length.
	For more details, checkout the <a href="{{ site.baseurl }}/assets/PDFs/POLICEd_RL_ArXiv.pdf" target="_blank" rel="noopener noreferrer">paper</a> and the <a href="https://iconlab.negarmehr.com/POLICEd-RL/" target="_blank" rel="noopener noreferrer">website</a>.
  </p>
  <div class="row">
    <div class="column">
	    <img src="{{site.baseurl}}/assets/gifs/POLICEd_2D_gif.gif" alt="animated" class="center_img"/>
    </div>
    <div class="column">
	    <img src="{{site.baseurl}}/assets/gifs/toy_baseline_ppt.gif" alt="animated" class="center_img"/>
    </div>
  </div>
	<div class="caption">
		Training of a policy to direct trajectories toward the target (cyan) whithout crossing the constraint line (red).<br>
		The POLICEd policy (left) is affine in the buffer region (green) and learns to push trajectories away from the constraint but not the baseline (right).
	</div>	

  
  
  
  
  <hr style="margin-top:1cm;">
  <h2>Resilience of Autonomous Systems</h2>
  
  <p>
	After docking to the International Space Station (ISS), the Nauka module suffered a software error causing its thrusters to misfire.
	In turn, these uncontrolled thrusters rotated the whole space station by 540° before being counteracted by other thrusters of the ISS.
	Motivated by such a scenario, my PhD thesis investigated the guaranteed resilience of autonomous systems to a similar class of malfunctions called partial loss of control authority over actuators.
	These malfunctions are characterized by actuators producing uncontrolled and undesirable outputs instead of following the controller’s commands.
	A loss of control authority can be caused, for instance, by a software bug as in the ISS example or by an adversarial takeover of some actuators of the system.
  </p>
  <p>  
	In this setting, I investigated the malfunctioning system's remaining capabilities to complete its mission in terms of resilient reachability and resilient trajectory tracking.
	I quantified the resilience of linear systems by comparing the reachability performance of the nominal dynamics with that of the worst-case malfunctioning dynamics.
	The resilience of driftless systems is quantified by the Maximax Minimax Quotient Theorem whose geometrical proof is illustrated on the video below.
  </p>	
	 
   <div class="videowrapper">
     <iframe src="https://www.youtube.com/embed/rjKzHyDJX40">
     </iframe>
   </div>
	
  <p>	
	I extended my resilience investigation to systems further inflicted with actuation delays preventing an immediate cancellation of the undesirable outputs.
	I illustrated my theory on a wide range of applications including an octocopter, a fighter jet model, and an orbital inspection mission illustrated by the following video.
  </p>

  <div class="videowrapper">
    <iframe src="https://www.youtube.com/embed/DQy8iNHyt7M">
	</iframe>
  </div>
  
  <p>
	For more details on resilience theory see <a href="{{ site.baseurl }}/assets/blog_posts/resilience">this blog post</a>.
  </p>




  <hr style="margin-top:1cm;">
  <h2>Astrodynamics work</h2>
  
  



  
  <hr style="margin-top:1cm;">
  <h2>Other Research Work</h2>
  
  
  <details>
    <summary>
      <div class="project-title">Transient Safety of Microgrids</div>
	  <div class="github-link">
        <a href="https://github.com/Jean-BaptisteBouvier/Microgrid-transient-safety" target="_blank" rel="noopener noreferrer"><img src="{{site.baseurl}}/assets/logos/GitHub_logo.png" width="25"></a>
      </div>
	</summary>
    <ul>
      <li>To ensure transient safety in inverter-based microgrids, we develop a set invariance-based distributed safety verification algorithm for each inverter module.
	  Applying Nagumo’s invariance condition, we construct a robust polynomial optimization problem to jointly search for safety-admissible set of control set-points and design parameters,
	  under allowable disturbances from neighbors.
	  We use sum-of-squares (SOS) programming to solve the verification problem and we perform numerical simulations using grid-forming inverters to illustrate the algorithm.</li>
      <li>This work has first been presented at the <a href="https://ieeexplore.ieee.org/document/9867323" target="_blank" rel="noopener noreferrer">2022 American Control Conference</a>.</li>
    </ul>

  </details>
  
  
  
  
  
  
<!-- Code to add a button return to top -->
<button onclick="topFunction()" id="topBtn" title="Go to top">Top</button>
<script src="{{ site.baseurl }}/assets/js/top_button.js" ></script>

  
</body>  

